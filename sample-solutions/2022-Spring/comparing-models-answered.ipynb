{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework - Comparing Models\n",
    "Run the following code and then answer the numbered questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_company</th>\n",
       "      <th>age</th>\n",
       "      <th>frequency_of_travel</th>\n",
       "      <th>department</th>\n",
       "      <th>commuting_distance</th>\n",
       "      <th>education</th>\n",
       "      <th>satisfaction_with_environment</th>\n",
       "      <th>gender</th>\n",
       "      <th>seniority_level</th>\n",
       "      <th>position</th>\n",
       "      <th>satisfaction_with_job</th>\n",
       "      <th>married_or_single</th>\n",
       "      <th>last_raise_pct</th>\n",
       "      <th>last_performance_rating</th>\n",
       "      <th>total_years_working</th>\n",
       "      <th>years_at_company</th>\n",
       "      <th>years_in_current_job</th>\n",
       "      <th>years_since_last_promotion</th>\n",
       "      <th>years_with_current_supervisor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>37</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>Sales</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "      <td>Sales Executive</td>\n",
       "      <td>3</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>39</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "      <td>Laboratory Technician</td>\n",
       "      <td>3</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>52</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Female</td>\n",
       "      <td>4</td>\n",
       "      <td>Manufacturing Director</td>\n",
       "      <td>4</td>\n",
       "      <td>Married</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>50</td>\n",
       "      <td>Non-Travel</td>\n",
       "      <td>Sales</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Female</td>\n",
       "      <td>2</td>\n",
       "      <td>Sales Executive</td>\n",
       "      <td>3</td>\n",
       "      <td>Married</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>44</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Male</td>\n",
       "      <td>2</td>\n",
       "      <td>Healthcare Representative</td>\n",
       "      <td>2</td>\n",
       "      <td>Single</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  left_company  age frequency_of_travel              department  \\\n",
       "0           No   37       Travel_Rarely                   Sales   \n",
       "1           No   39       Travel_Rarely  Research & Development   \n",
       "2           No   52   Travel_Frequently  Research & Development   \n",
       "3           No   50          Non-Travel                   Sales   \n",
       "4           No   44       Travel_Rarely  Research & Development   \n",
       "\n",
       "   commuting_distance  education  satisfaction_with_environment  gender  \\\n",
       "0                  16          4                              4    Male   \n",
       "1                   3          2                              3    Male   \n",
       "2                  25          4                              3  Female   \n",
       "3                   1          3                              4  Female   \n",
       "4                   4          3                              4    Male   \n",
       "\n",
       "   seniority_level                   position  satisfaction_with_job  \\\n",
       "0                2            Sales Executive                      3   \n",
       "1                2      Laboratory Technician                      3   \n",
       "2                4     Manufacturing Director                      4   \n",
       "3                2            Sales Executive                      3   \n",
       "4                2  Healthcare Representative                      2   \n",
       "\n",
       "  married_or_single  last_raise_pct  last_performance_rating  \\\n",
       "0          Divorced              19                        3   \n",
       "1          Divorced              15                        3   \n",
       "2           Married              22                        4   \n",
       "3           Married              12                        3   \n",
       "4            Single              12                        3   \n",
       "\n",
       "   total_years_working  years_at_company  years_in_current_job  \\\n",
       "0                    9                 1                     0   \n",
       "1                   11                10                     8   \n",
       "2                   31                 9                     8   \n",
       "3                   19                18                     7   \n",
       "4                   10                 5                     2   \n",
       "\n",
       "   years_since_last_promotion  years_with_current_supervisor  \n",
       "0                           0                              0  \n",
       "1                           0                              7  \n",
       "2                           0                              0  \n",
       "3                           0                             13  \n",
       "4                           2                              3  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "df = pd.read_csv('employee-turnover-balanced.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 19 columns):\n",
      " #   Column                         Non-Null Count  Dtype \n",
      "---  ------                         --------------  ----- \n",
      " 0   left_company                   1000 non-null   object\n",
      " 1   age                            1000 non-null   int64 \n",
      " 2   frequency_of_travel            1000 non-null   object\n",
      " 3   department                     1000 non-null   object\n",
      " 4   commuting_distance             1000 non-null   int64 \n",
      " 5   education                      1000 non-null   int64 \n",
      " 6   satisfaction_with_environment  1000 non-null   int64 \n",
      " 7   gender                         1000 non-null   object\n",
      " 8   seniority_level                1000 non-null   int64 \n",
      " 9   position                       1000 non-null   object\n",
      " 10  satisfaction_with_job          1000 non-null   int64 \n",
      " 11  married_or_single              1000 non-null   object\n",
      " 12  last_raise_pct                 1000 non-null   int64 \n",
      " 13  last_performance_rating        1000 non-null   int64 \n",
      " 14  total_years_working            1000 non-null   int64 \n",
      " 15  years_at_company               1000 non-null   int64 \n",
      " 16  years_in_current_job           1000 non-null   int64 \n",
      " 17  years_since_last_promotion     1000 non-null   int64 \n",
      " 18  years_with_current_supervisor  1000 non-null   int64 \n",
      "dtypes: int64(13), object(6)\n",
      "memory usage: 148.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: left_company\n",
      "Features:\n",
      "age,\tfrequency_of_travel,\tdepartment,\tcommuting_distance,\teducation,\tsatisfaction_with_environment,\tgender,\tseniority_level,\tposition,\tsatisfaction_with_job,\tmarried_or_single,\tlast_raise_pct,\tlast_performance_rating,\ttotal_years_working,\tyears_at_company,\tyears_in_current_job,\tyears_since_last_promotion,\tyears_with_current_supervisor\n",
      "\n",
      "\n",
      "Training examples: 800\n",
      "Test examples: 200\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "targetCol = 'left_company'\n",
    "featureCols = [x for x in df.columns if x != targetCol]\n",
    "\n",
    "y = df[targetCol]\n",
    "X = df[featureCols]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 123)\n",
    "\n",
    "print(f'Target: {targetCol}')\n",
    "print('Features:')\n",
    "print(*featureCols, sep=',\\t')\n",
    "print('\\n')\n",
    "print(f'Training examples: {x_train.shape[0]:,}')\n",
    "print(f'Test examples: {x_test.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "- Set-up a feature processing pipeline using the training data.  \n",
    "- While the data doesn't contain missing values, assume the test data could.  \n",
    "- Use the below lists to split the features that should be treated as numerical and categorical variables.  \n",
    "- I would recommend printing out verification output to verify the pipeline transforms the data as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericalFeatures = ['age', 'commuting_distance', 'last_raise_pct', 'total_years_working', 'years_at_company',\n",
    "                    'years_in_current_job', 'years_since_last_promotion', 'years_with_current_supervisor']\n",
    "\n",
    "categoricalFeatures = ['frequency_of_travel', 'department', 'education', 'satisfaction_with_environment',\n",
    "                       'gender', 'seniority_level', 'position', 'satisfaction_with_job', 'married_or_single',\n",
    "                      'last_performance_rating']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 38)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_pipe = Pipeline([('impute', SimpleImputer(strategy='median')), \n",
    "                     ('scale', StandardScaler())]\n",
    "                   )\n",
    "\n",
    "cat_pipe = Pipeline([('impute', SimpleImputer(strategy='most_frequent')), \n",
    "                     ('ohe', OneHotEncoder(drop='first'))]\n",
    "                   )\n",
    "\n",
    "proc = ColumnTransformer(transformers=[('nums', num_pipe, numericalFeatures),\n",
    "                                      ('cats', cat_pipe, categoricalFeatures)]\n",
    "                        )\n",
    "\n",
    "proc.fit_transform(x_train).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "- Using cross-validation evaluate a logistic regression model, evaluate three (4) different regularization strengths with `l2`, with one model containing no regularization.  \n",
    "- Which regularization strength would be ideal for logistic regression?\n",
    "- Use as much code as you need to defend your opinion.  \n",
    "- Comment on your rationale for your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     500\n",
       "Yes    500\n",
       "Name: left_company, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The data is balanced, so will use accuracy to \"optimize\" our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('fp',\n",
       "                 ColumnTransformer(transformers=[('nums',\n",
       "                                                  Pipeline(steps=[('impute',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('scale',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['age', 'commuting_distance',\n",
       "                                                   'last_raise_pct',\n",
       "                                                   'total_years_working',\n",
       "                                                   'years_at_company',\n",
       "                                                   'years_in_current_job',\n",
       "                                                   'years_since_last_promotion',\n",
       "                                                   'years_with_current_supervisor']),\n",
       "                                                 ('cats',\n",
       "                                                  Pipeline(steps=[('impute',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(drop='first'))]),\n",
       "                                                  ['frequency_of_travel',\n",
       "                                                   'department', 'education',\n",
       "                                                   'satisfaction_with_environment',\n",
       "                                                   'gender', 'seniority_level',\n",
       "                                                   'position',\n",
       "                                                   'satisfaction_with_job',\n",
       "                                                   'married_or_single',\n",
       "                                                   'last_performance_rating'])])),\n",
       "                ('lg', LogisticRegression(C=0.1, solver='newton-cg'))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "params = {'lg__C': [0.01, 0.1, 1, 10], 'lg__penalty':['l2', 'none']}\n",
    "\n",
    "lg_model = Pipeline([('fp', proc), \n",
    "                     ('lg', LogisticRegression(solver='newton-cg'))\n",
    "                    ])\n",
    "\n",
    "lg = GridSearchCV(lg_model, param_grid=params)\n",
    "lg = lg.fit(x_train, y_train)\n",
    "\n",
    "lg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7175"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Best validation accuracy is about 72%. Need to determine variances in the CV results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization: 0.01, lower acc: 63.06%, mean acc:66.88%, upper acc: 70.69%\n",
      "No Regularization:, lower acc: 69.57%, mean acc: 71.12%, upper acc: 72.68%\n",
      "Regularization: 0.1, lower acc: 68.32%, mean acc:71.75%, upper acc: 75.18%\n",
      "Regularization: 1, lower acc: 68.50%, mean acc:70.75%, upper acc: 73.00%\n",
      "Regularization: 10, lower acc: 68.86%, mean acc:70.62%, upper acc: 72.39%\n"
     ]
    }
   ],
   "source": [
    "lg_results = list(zip(lg.cv_results_['params'],\n",
    "                      lg.cv_results_['mean_test_score'],\n",
    "                      lg.cv_results_['std_test_score']\n",
    "                     )\n",
    "                 )\n",
    "\n",
    "noRegCount = 0\n",
    "for i in lg_results:\n",
    "    x = i[0]['lg__C']\n",
    "    y = i[0]['lg__penalty']\n",
    "    if y == 'none' and noRegCount < 1:\n",
    "        print(f'No Regularization:, lower acc: {(i[1]-i[2]):.2%}, mean acc: {i[1]:.2%}, upper acc: {(i[1]+i[2]):.2%}')\n",
    "        noRegCount += 1\n",
    "    if y == 'l2':\n",
    "        print(f'Regularization: {x}, lower acc: {(i[1]-i[2]):.2%}, mean acc:{i[1]:.2%}, upper acc: {(i[1]+i[2]):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `C=0.1` provides the best model with the training and validation data as it maximizes accuracy. It does have a slightly higher standard deviation, however, the lower bound range is about the same of `C=10` but its upper range is higher. Its fit time is also in-line with the other models so computationally no different than the other choices. Will need to search around `0.1` to determine if that is the best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('fp',\n",
       "                 ColumnTransformer(transformers=[('nums',\n",
       "                                                  Pipeline(steps=[('impute',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('scale',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['age', 'commuting_distance',\n",
       "                                                   'last_raise_pct',\n",
       "                                                   'total_years_working',\n",
       "                                                   'years_at_company',\n",
       "                                                   'years_in_current_job',\n",
       "                                                   'years_since_last_promotion',\n",
       "                                                   'years_with_current_supervisor']),\n",
       "                                                 ('cats',\n",
       "                                                  Pipeline(steps=[('impute',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(drop='first'))]),\n",
       "                                                  ['frequency_of_travel',\n",
       "                                                   'department', 'education',\n",
       "                                                   'satisfaction_with_environment',\n",
       "                                                   'gender', 'seniority_level',\n",
       "                                                   'position',\n",
       "                                                   'satisfaction_with_job',\n",
       "                                                   'married_or_single',\n",
       "                                                   'last_performance_rating'])])),\n",
       "                ('lg', LogisticRegression(C=0.1, solver='newton-cg'))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "params = {'lg__C': [0.05, 0.08, 0.1, 0.12, 0.15], 'lg__penalty':['l2', 'none']}\n",
    "\n",
    "lg_model = Pipeline([('fp', proc), \n",
    "                     ('lg', LogisticRegression(solver='newton-cg'))\n",
    "                    ])\n",
    "\n",
    "lg = GridSearchCV(lg_model, param_grid=params)\n",
    "lg = lg.fit(x_train, y_train)\n",
    "\n",
    "lg.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization: 0.05, lower acc: 68.03%, mean acc:70.62%, upper acc: 73.22%\n",
      "No Regularization:, lower acc: 69.57%, mean acc: 71.12%, upper acc: 72.68%\n",
      "Regularization: 0.08, lower acc: 68.83%, mean acc:71.38%, upper acc: 73.92%\n",
      "Regularization: 0.1, lower acc: 68.32%, mean acc:71.75%, upper acc: 75.18%\n",
      "Regularization: 0.12, lower acc: 68.50%, mean acc:71.62%, upper acc: 74.75%\n",
      "Regularization: 0.15, lower acc: 68.46%, mean acc:71.37%, upper acc: 74.29%\n"
     ]
    }
   ],
   "source": [
    "lg_results = list(zip(lg.cv_results_['params'],\n",
    "                      lg.cv_results_['mean_test_score'],\n",
    "                      lg.cv_results_['std_test_score']\n",
    "                     )\n",
    "                 )\n",
    "\n",
    "noRegCount = 0\n",
    "for i in lg_results:\n",
    "    x = i[0]['lg__C']\n",
    "    y = i[0]['lg__penalty']\n",
    "    if y == 'none' and noRegCount < 1:\n",
    "        print(f'No Regularization:, lower acc: {(i[1]-i[2]):.2%}, mean acc: {i[1]:.2%}, upper acc: {(i[1]+i[2]):.2%}')\n",
    "        noRegCount += 1\n",
    "    if y == 'l2':\n",
    "        print(f'Regularization: {x}, lower acc: {(i[1]-i[2]):.2%}, mean acc:{i[1]:.2%}, upper acc: {(i[1]+i[2]):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `C=0.1` still showing the \"best\" performance in relation to the accuracy after a secondary targeted hyperparameter search. We'll conclude that is the best parameter to utilize for our modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.78      0.72      0.75       101\n",
      "         Yes       0.74      0.79      0.76        99\n",
      "\n",
      "    accuracy                           0.76       200\n",
      "   macro avg       0.76      0.76      0.75       200\n",
      "weighted avg       0.76      0.76      0.75       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lg_pred_evt = lg.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, lg_pred_evt, labels=['No','Yes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Our estimate of out-of-sample performance is 76% accuracy, which is slightly higher than what we saw on the validation sets during cross-validation, however, as shown below, it performs similar to the entire training data, so no concerns on overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.76      0.73      0.75       399\n",
      "         Yes       0.74      0.77      0.76       401\n",
      "\n",
      "    accuracy                           0.75       800\n",
      "   macro avg       0.75      0.75      0.75       800\n",
      "weighted avg       0.75      0.75      0.75       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lg_pred_evt = lg.predict(x_train)\n",
    "\n",
    "print(classification_report(y_train, lg_pred_evt, labels=['No','Yes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "- Using cross-validation, evaluate a decision tree. Consider at least 3 different `max_depth` and 3 different `min_samples_split`. Use a fraction for the `min_samples_split`.  \n",
    "- Which combination seems to be the ideal for the decision tree? Use performance from the cross-validation and test sets to defend your opinion.  \n",
    "- Use as much code a you need to defend your opinion.  \n",
    "- Comment on your rationale for your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('fp',\n",
       "                 ColumnTransformer(transformers=[('nums',\n",
       "                                                  Pipeline(steps=[('impute',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('scale',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['age', 'commuting_distance',\n",
       "                                                   'last_raise_pct',\n",
       "                                                   'total_years_working',\n",
       "                                                   'years_at_company',\n",
       "                                                   'years_in_current_job',\n",
       "                                                   'years_since_last_promotion',\n",
       "                                                   'years_with_current_supervisor']),\n",
       "                                                 ('cats',\n",
       "                                                  Pipeline(steps=[('impute',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(drop='first'))]),\n",
       "                                                  ['frequency_of_travel',\n",
       "                                                   'department', 'education',\n",
       "                                                   'satisfaction_with_environment',\n",
       "                                                   'gender', 'seniority_level',\n",
       "                                                   'position',\n",
       "                                                   'satisfaction_with_job',\n",
       "                                                   'married_or_single',\n",
       "                                                   'last_performance_rating'])])),\n",
       "                ('dt',\n",
       "                 DecisionTreeClassifier(max_depth=16, min_samples_split=0.01))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'dt__max_depth': [2, 4, 8, 12, 16],\n",
    "          'dt__min_samples_split':[0.01, 0.05, 0.10]\n",
    "         }\n",
    "\n",
    "dt_model = Pipeline([('fp', proc), ('dt', DecisionTreeClassifier())])\n",
    "\n",
    "dt = GridSearchCV(dt_model, param_grid=params)\n",
    "dt = dt.fit(x_train, y_train)\n",
    "\n",
    "dt.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72875"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth/Min Split: 2, 1.00%: 55.43% < 60.75% > 66.07%\n",
      "Depth/Min Split: 2, 5.00%: 55.43% < 60.75% > 66.07%\n",
      "Depth/Min Split: 2, 10.00%: 55.43% < 60.75% > 66.07%\n",
      "Depth/Min Split: 4, 1.00%: 59.85% < 63.38% > 66.90%\n",
      "Depth/Min Split: 4, 5.00%: 59.91% < 63.25% > 66.59%\n",
      "Depth/Min Split: 4, 10.00%: 59.72% < 62.87% > 66.03%\n",
      "Depth/Min Split: 8, 1.00%: 65.85% < 68.62% > 71.40%\n",
      "Depth/Min Split: 8, 5.00%: 64.61% < 67.25% > 69.89%\n",
      "Depth/Min Split: 8, 10.00%: 59.95% < 64.12% > 68.30%\n",
      "Depth/Min Split: 12, 1.00%: 69.17% < 72.00% > 74.83%\n",
      "Depth/Min Split: 12, 5.00%: 65.77% < 68.25% > 70.73%\n",
      "Depth/Min Split: 12, 10.00%: 59.92% < 64.00% > 68.08%\n",
      "Depth/Min Split: 16, 1.00%: 69.90% < 72.88% > 75.85%\n",
      "Depth/Min Split: 16, 5.00%: 66.11% < 68.50% > 70.89%\n",
      "Depth/Min Split: 16, 10.00%: 59.95% < 64.12% > 68.30%\n"
     ]
    }
   ],
   "source": [
    "dt_results = list(zip(dt.cv_results_['params'],\n",
    "                      dt.cv_results_['mean_test_score'],\n",
    "                      dt.cv_results_['std_test_score']\n",
    "                     )\n",
    "                 )\n",
    "\n",
    "for i in dt_results:\n",
    "    x = i[0]['dt__max_depth']\n",
    "    y = i[0]['dt__min_samples_split']\n",
    "    print(f'Depth/Min Split: {x}, {y:.2%}: {(i[1]-i[2]):.2%} < {i[1]:.2%} > {(i[1]+i[2]):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Depending on the run, `max_splits` of 12 or 16 could be preferred, both also using 1% minimum sample split. Each is near identical in terms of the lower, mean, and upper bound performance on the validation sets. Since the option with a `max_depth=12`, that will be our choice, defaulting to the simpliest model.\n",
    "\n",
    "> We'll perform a more targeted search to determine a better value for the max splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('fp',\n",
       "                 ColumnTransformer(transformers=[('nums',\n",
       "                                                  Pipeline(steps=[('impute',\n",
       "                                                                   SimpleImputer(strategy='median')),\n",
       "                                                                  ('scale',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['age', 'commuting_distance',\n",
       "                                                   'last_raise_pct',\n",
       "                                                   'total_years_working',\n",
       "                                                   'years_at_company',\n",
       "                                                   'years_in_current_job',\n",
       "                                                   'years_since_last_promotion',\n",
       "                                                   'years_with_current_supervisor']),\n",
       "                                                 ('cats',\n",
       "                                                  Pipeline(steps=[('impute',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(drop='first'))]),\n",
       "                                                  ['frequency_of_travel',\n",
       "                                                   'department', 'education',\n",
       "                                                   'satisfaction_with_environment',\n",
       "                                                   'gender', 'seniority_level',\n",
       "                                                   'position',\n",
       "                                                   'satisfaction_with_job',\n",
       "                                                   'married_or_single',\n",
       "                                                   'last_performance_rating'])])),\n",
       "                ('dt',\n",
       "                 DecisionTreeClassifier(max_depth=14, min_samples_split=0.01))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'dt__max_depth': [10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
    "          'dt__min_samples_split':[0.01, 0.02, 0.03]\n",
    "         }\n",
    "\n",
    "dt_model = Pipeline([('fp', proc), ('dt', DecisionTreeClassifier())])\n",
    "\n",
    "dt = GridSearchCV(dt_model, param_grid=params)\n",
    "dt = dt.fit(x_train, y_train)\n",
    "\n",
    "dt.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7325"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth/Min Split: 10, 1.00%: 66.22% < 69.88% > 73.53%\n",
      "Depth/Min Split: 10, 2.00%: 67.30% < 69.62% > 71.95%\n",
      "Depth/Min Split: 10, 3.00%: 66.61% < 69.12% > 71.64%\n",
      "Depth/Min Split: 11, 1.00%: 69.22% < 72.00% > 74.78%\n",
      "Depth/Min Split: 11, 2.00%: 69.77% < 72.12% > 74.48%\n",
      "Depth/Min Split: 11, 3.00%: 67.58% < 70.12% > 72.67%\n",
      "Depth/Min Split: 12, 1.00%: 69.80% < 72.38% > 74.95%\n",
      "Depth/Min Split: 12, 2.00%: 69.92% < 71.75% > 73.58%\n",
      "Depth/Min Split: 12, 3.00%: 68.05% < 70.38% > 72.70%\n",
      "Depth/Min Split: 13, 1.00%: 70.06% < 73.12% > 76.19%\n",
      "Depth/Min Split: 13, 2.00%: 70.25% < 71.88% > 73.50%\n",
      "Depth/Min Split: 13, 3.00%: 68.33% < 70.88% > 73.42%\n",
      "Depth/Min Split: 14, 1.00%: 70.68% < 73.25% > 75.82%\n",
      "Depth/Min Split: 14, 2.00%: 68.53% < 70.62% > 72.72%\n",
      "Depth/Min Split: 14, 3.00%: 67.02% < 70.00% > 72.98%\n",
      "Depth/Min Split: 15, 1.00%: 69.78% < 72.00% > 74.22%\n",
      "Depth/Min Split: 15, 2.00%: 70.01% < 72.12% > 74.24%\n",
      "Depth/Min Split: 15, 3.00%: 67.32% < 70.38% > 73.43%\n",
      "Depth/Min Split: 16, 1.00%: 69.03% < 72.50% > 75.97%\n",
      "Depth/Min Split: 16, 2.00%: 69.74% < 72.38% > 75.01%\n",
      "Depth/Min Split: 16, 3.00%: 68.30% < 71.00% > 73.70%\n",
      "Depth/Min Split: 17, 1.00%: 71.03% < 73.25% > 75.47%\n",
      "Depth/Min Split: 17, 2.00%: 70.16% < 72.37% > 74.59%\n",
      "Depth/Min Split: 17, 3.00%: 67.92% < 70.62% > 73.33%\n",
      "Depth/Min Split: 18, 1.00%: 70.01% < 73.00% > 75.99%\n",
      "Depth/Min Split: 18, 2.00%: 68.81% < 71.25% > 73.69%\n",
      "Depth/Min Split: 18, 3.00%: 68.62% < 70.88% > 73.13%\n"
     ]
    }
   ],
   "source": [
    "dt_results = list(zip(dt.cv_results_['params'],\n",
    "                      dt.cv_results_['mean_test_score'],\n",
    "                      dt.cv_results_['std_test_score']\n",
    "                     )\n",
    "                 )\n",
    "\n",
    "for i in dt_results:\n",
    "    x = i[0]['dt__max_depth']\n",
    "    y = i[0]['dt__min_samples_split']\n",
    "    print(f'Depth/Min Split: {x}, {y:.2%}: {(i[1]-i[2]):.2%} < {i[1]:.2%} > {(i[1]+i[2]):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> After a secondary search, we find that a `max_depth=18` is a better value, with higher accuracy values. The `min_samples_split=0.01` is still the corresponding choice for that hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.76      0.68      0.72       101\n",
      "         Yes       0.71      0.78      0.74        99\n",
      "\n",
      "    accuracy                           0.73       200\n",
      "   macro avg       0.73      0.73      0.73       200\n",
      "weighted avg       0.73      0.73      0.73       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dt_pred_evt = dt.predict(x_test)\n",
    "\n",
    "print(classification_report(y_test, dt_pred_evt, labels=['No','Yes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Out-of-sample performance is expected to be 73% accuracy, with 78% recall and 71% precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.93      0.96      0.95       399\n",
      "         Yes       0.96      0.93      0.94       401\n",
      "\n",
      "    accuracy                           0.94       800\n",
      "   macro avg       0.95      0.95      0.94       800\n",
      "weighted avg       0.95      0.94      0.94       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dt_pred_evt = dt.predict(x_train)\n",
    "\n",
    "print(classification_report(y_train, dt_pred_evt, labels=['No','Yes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> However, comparing the performance of the test and training sets, it seems there could be significant overfitting. The above shows 96% accuracy in the training set, which is 20 points higher than the accuracy on the test set. We should be cautious given this and monitor the consistency of the performance results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "- Compare the best logistic regression and best decision tree and decide which is superior.  \n",
    "- Use as much code as you need to defend your opinion. Use performance from the cross-validation and test sets to defend your opinion.  \n",
    "- Comment on your rationale for your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression, Training: 0.7525\n",
      "Logistic Regression, Test: 0.755\n"
     ]
    }
   ],
   "source": [
    "print(f'Logistic Regression, Training: {lg.score(x_train, y_train)}')\n",
    "print(f'Logistic Regression, Test: {lg.score(x_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree, Training: 0.945\n",
      "Decision Tree, Test: 0.73\n"
     ]
    }
   ],
   "source": [
    "print(f'Decision Tree, Training: {dt.score(x_train, y_train)}')\n",
    "print(f'Decision Tree, Test: {dt.score(x_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Logistic regression has an accuracy on the test set, so strictly by that metric we would prefer that model over the decision tree. Additionally, the decision tree is overfitting, as evidenced by the drop-off in accuracy from the training to test set, i.e., the decision tree is capturing more of the random error term in the training data than the logistic regression, which is something we want to avoid.\n",
    "\n",
    "> To further look at the trade-offs, we can examine the ROC curves, which shows the trade-offs between the false positive rate (FPR) and true positive rate (TPR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "lg_pred = lg.predict_proba(x_test)\n",
    "dt_pred = dt.predict_proba(x_test)\n",
    "\n",
    "lg_fpr, lg_tpr, lg_thr = roc_curve(y_test, lg_pred[:,1], pos_label='Yes')\n",
    "dt_fpr, dt_tpr, dt_thr = roc_curve(y_test, dt_pred[:,1], pos_label='Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lg_fpr, lg_tpr)\n",
    "plt.plot(dt_fpr, dt_tpr)\n",
    "plt.plot(lg_fpr, lg_fpr, color='green', linestyle='dashed')\n",
    "plt.title('ROC Curve', loc='left')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(['Logistic Regression', 'Decision Tree'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is a portion of the ROC curve whether we'd prefer the decision tree, given its lower FPR and higher TPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FPR</th>\n",
       "      <th>TRP</th>\n",
       "      <th>THRES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.356436</td>\n",
       "      <td>0.898990</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.356436</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.316832</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.316832</td>\n",
       "      <td>0.757576</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.297030</td>\n",
       "      <td>0.737374</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.287129</td>\n",
       "      <td>0.717172</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.277228</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        FPR       TRP     THRES\n",
       "8  0.356436  0.898990  0.428571\n",
       "7  0.356436  0.858586  0.500000\n",
       "6  0.316832  0.777778  0.666667\n",
       "5  0.316832  0.757576  0.750000\n",
       "4  0.297030  0.737374  0.857143\n",
       "3  0.287129  0.717172  0.882353\n",
       "2  0.277228  0.666667  0.888889"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtr = (\n",
    "pd.DataFrame(list(zip(dt_fpr, dt_tpr, dt_thr)), columns=['FPR','TRP','THRES'])\n",
    ".sort_values(by='THRES')    \n",
    ")\n",
    "\n",
    "dtr.query('THRES > 0.4 and THRES < 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Overall, the logistic regression appears to be the better model. The accuracy on the test set is higher and the ROC is generally better at most threshold values. However, we can set the threshold for the `Decision Tree` at `0.428` and have a better trade-off between false-positives and true-positives. Eyeballing the ROC chart, for the `Logistic Regression` we might want to consider a threshold of `0.3` since that seems to be the point where we begin to see slowing gains in the FPR/TPR trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.85      0.34      0.48       101\n",
      "         Yes       0.58      0.94      0.72        99\n",
      "\n",
      "    accuracy                           0.64       200\n",
      "   macro avg       0.72      0.64      0.60       200\n",
      "weighted avg       0.72      0.64      0.60       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lg_pred_evt = lg.predict_proba(x_test)\n",
    "lg_pred_evt = np.where(lg_pred_evt[:,1] < 0.3, 'No', 'Yes')\n",
    "\n",
    "print(classification_report(y_test, lg_pred_evt, labels=['No','Yes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          No       0.82      0.64      0.72       101\n",
      "         Yes       0.70      0.86      0.77        99\n",
      "\n",
      "    accuracy                           0.75       200\n",
      "   macro avg       0.76      0.75      0.75       200\n",
      "weighted avg       0.76      0.75      0.75       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_pred_evt = dt.predict_proba(x_test)\n",
    "dt_pred_evt = np.where(dt_pred_evt[:,1] < 0.43, 'No', 'Yes')\n",
    "\n",
    "print(classification_report(y_test, dt_pred_evt, labels=['No','Yes']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Confirms the lower threshold for the decision tree creates a higher recall and not a significant trade-off to precision; a similar experiment for logistic regression shows a even better recall, but a larger impact to accuracy. If that trade-off is acceptable from a business perspective, we could recommend the `decision tree` with the non-standard threshold value given the apparent \"free lunch\". However, due to the overfitting that we suspect the decision tree is doing, I would be cautious in choosing that without additional testing to determine the stability of the performance results. As of now, the logistic regression seems to provide at least comparable, if not better, performance, than the decision tree, without overfitting, so we can reasonably defend choosing the logistic regression with a `C=0.01` regularization value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- No errors, warnings -- shouldn't see those in the EDA or projects.  \n",
    "- Be thorough -- there isn't a lot of additional work to be thorough.  \n",
    "- Be creative  \n",
    "- Use markdowns to annotate what you are seeing - I don't know if you understand the output without discussing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda3e66817595a24f3b851fad3315f1c145"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
