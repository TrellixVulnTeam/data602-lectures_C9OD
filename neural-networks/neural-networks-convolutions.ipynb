{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Networks Introduction\n",
    "Generally follows and uses `Python Machine Learning 3rd Edition,Raschka, Chapter 15`.\n",
    "\n",
    "- This will introduce basic concepts regarding neural networks and is by no-means comprehensive.  \n",
    "- Neural networks can cover multiple full courses, so this only scratches the surface.  \n",
    "- See the readings and resources section for resources for more information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Networks - What are they?\n",
    "First proposed by [Yann LeCun](https://papers.nips.cc/paper/1989/hash/53c3bce66e43be4f209556518c2fcb54-Abstract.html).\n",
    "\n",
    "- Deep networks of traditional hidden units grow $(size_{l-1}+1)size_l$ parameters, e.g., adding 1,000-unit layer to an existing NN will add more than 1,000,000 parameters. That would be challenging to solve computationally.  \n",
    "- Common in image processing as it tries to mimic the visual cortex.  \n",
    "    - We know the there are different layers that process information.  \n",
    "    - Some layers detect edges, outlines, and straight lines.  \n",
    "    - Other layers allow us to recognize more complex shapes.\n",
    "- The cortex isolates particular parts of the image, e.g., which section has the cat, which section of the cat has the tail, ....  \n",
    "- With images, most nieghboring pixels represent the same thing, e.g., the individual blue pixels in the sky.  \n",
    "- The edges of the objects, e.g., blue sky and white clouds, will generally be the only parts of the image with different neighbors.  \n",
    "- Can be viewed as feature extractors, e.g., edges and blobs of images.  \n",
    "- For color images, generally use 3 versions of the image, one in each of the primary color scales (RGB).  \n",
    "\n",
    "<img src='files/diagrams/architecture-cnn-en.jpeg' style='height: 500 px'>\n",
    "\n",
    "[Image Source: Stanford CS230 Convolutional Neural Networks cheatsheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)\n",
    "\n",
    "- [Convolutional layers](https://keras.io/api/layers/convolution_layers/convolution2d/) detect features by moving across the images in squares and multiplying them against a filter; think of it as scanning the image and \"summarizing\" the blocks.  \n",
    "- The filter is specified in the convolution layers, and the matrix of weights are learned. These individual filters are concatonated into a kernel (i.e., matrix of weights).    \n",
    "- Then it (usually) pools these scans, usually by taking the average or maximum - these aren't learnable, they just summarize the convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "Sometimes referred as feature extractors. The layers and filters are recoverable so you could look to understand which filter is identitying certain attributes.\n",
    "\n",
    "- Low-level features are extracted first.  \n",
    "- Later layers use the above within the network.\n",
    "\n",
    "<img src='diagrams/14_01_feature_map.png'>\n",
    "\n",
    "[Image source: Python Machine Learning 3rd Edition, page 519](https://github.com/rasbt/machine-learning-book/blob/main/ch14/figures/14_01.png)\n",
    "\n",
    "- Local block of pixels are `local receptive field`. These are have `sparse connectivity` to pixels far away, e.g., the pixels near the dog's face and pixels in the grass field.  \n",
    "- Filters and pooling extract the filters for the model. The features are all `latent` or `salient` and convolution networks are effective at extracting them, given enough examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Math of Convolution (discrete example)\n",
    "\n",
    "$$\n",
    "y = x * w \\rightarrow y[i] = \\sum{x[i-k]w[k]}\n",
    "$$\n",
    "\n",
    "- $x$: input vector. \n",
    "- $w$: filter, these are learned parameters\n",
    "- $y$: convolution\n",
    "\n",
    "See pages 522 - 528 for more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs, Padding, and Strides\n",
    "\n",
    "- The inputs (`n x n` matrix of local pixels) are isolated and effectively scanned.  \n",
    "- Padding will pad zeroes around the raw the area where the filter is being applied.  \n",
    "    - This can help define the boundaries between shapes/objects.  \n",
    "    - This also gives the chance for each pixel to be at the center when the filter is being applied and help with feature detection in those areas of the image.\n",
    "- Strides are how big the step is for the next patch of pixels.  \n",
    "    - A larger stride will decrease the size of the output and create a similar representation and can be used for compression.  \n",
    "    - Generally, strides are down symmetrically.\n",
    "\n",
    "\n",
    "\n",
    "<img src='files/diagrams/padding.png'>\n",
    "\n",
    "[Source: Hundred Page Machine Learning Book](https://www.dropbox.com/s/uh48e6wjs4w13t5/Chapter6.pdf?dl=0)\n",
    "\n",
    "\n",
    "<img src='files/diagrams/strides.png'>\n",
    "\n",
    "Keras has options for both of these hyperparameters. [See the documentation for the Conv2D layer](https://keras.io/api/layers/convolution_layers/convolution2d/). Padding is implemented in its own layer, [see the documentation for it](https://keras.io/api/layers/reshaping_layers/zero_padding2d/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling (Aggregating)\n",
    "\n",
    "<img src='diagrams/14_08_pooling.png' style='width: 700px'>\n",
    "\n",
    "[Image Source: Python Machine Learning 3rd Edition, page 531](https://github.com/rasbt/machine-learning-book/blob/main/ch14/figures/14_08.png)\n",
    "\n",
    "- Done on the filters.  \n",
    "- Helps reduces variance among the local pixels between strides.  \n",
    "- Creates more robust features since there is less sensitivity to variation.  \n",
    "- Since it aggregates the convolutions, it leads to less features, which increases computational efficiency.  \n",
    "- Generally is included after convolution step, however, there have been convolutional networks built without them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channels\n",
    "\n",
    "- May have to work with multiple data layers, that represent differing intensities of color.  \n",
    "- $X_{N_1xN_2xC_in}$, where $C_{in}=3$ for RGB colors.  \n",
    "- scikit-learn's MNIST data is greyscale, or $C_{in}=1$.  \n",
    "- Convolutions are done separately for each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "- $l_2$ regularization.  \n",
    "- Dropout (see introduction neural network lecture).  \n",
    "    - Typically done on the later layers.  \n",
    "    - Randomly drop neurons and the non-dropped account for the missing ones.  \n",
    "    - Forces simplicity.  \n",
    "    - Also a form of ensemble since it randomly drops and then averages.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "Architecture being used: \n",
    "\n",
    "<img src='diagrams/14_12_arch.png'>\n",
    "\n",
    "[Image Source: Python Machine Learning 3rd Edition, Page 542](https://github.com/rasbt/machine-learning-book/tree/main/ch14)\n",
    "\n",
    "> We will also include dropout in the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.257845\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "\n",
    "st = datetime.datetime.now()\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "# 28 x 28 = 784\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "en = datetime.datetime.now()\n",
    "el = en - st\n",
    "\n",
    "print(f'Elapsed time: {el}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60,000 train samples\n",
      "10,000 test samples\n",
      "Columns in y: 10\n"
     ]
    }
   ],
   "source": [
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "print(f'{x_train.shape[0]:,} train samples')\n",
    "print(f'{x_test.shape[0]:,} test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f'Columns in y: {y_train.shape[1]:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We exploded the categories, so we can use the cross entropy categorical loss. If we hadn't done this, we would have needed to use the `sparse` version.\n",
    "\n",
    "[Keras Loss Functions](https://keras.io/api/losses/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Convolutional\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 64)          51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 62,346\n",
      "Trainable params: 62,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(filters=32, kernel_size=(5, 5), activation=\"relu\", strides=(1,1)),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(filters=64, kernel_size=(5, 5), activation=\"relu\", strides=(1,1)),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ], name= 'Convolutional'\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note the difference in the number of parameters the model needs to learn\n",
    "- 34,826 for the Convolutional despite several more layers than the deep MLP.  \n",
    "- 42,000 for the network with two hidden 50-unit layers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/15\n",
      "54000/54000 [==============================] - 15s 269us/sample - loss: 0.3124 - accuracy: 0.9056 - val_loss: 0.0653 - val_accuracy: 0.9837\n",
      "Epoch 2/15\n",
      "54000/54000 [==============================] - 14s 261us/sample - loss: 0.0924 - accuracy: 0.9734 - val_loss: 0.0469 - val_accuracy: 0.9882\n",
      "Epoch 3/15\n",
      "54000/54000 [==============================] - 16s 295us/sample - loss: 0.0654 - accuracy: 0.9801 - val_loss: 0.0382 - val_accuracy: 0.9900\n",
      "Epoch 4/15\n",
      "54000/54000 [==============================] - 14s 253us/sample - loss: 0.0533 - accuracy: 0.9835 - val_loss: 0.0379 - val_accuracy: 0.9890\n",
      "Epoch 5/15\n",
      "54000/54000 [==============================] - 13s 241us/sample - loss: 0.0472 - accuracy: 0.9848 - val_loss: 0.0324 - val_accuracy: 0.9908\n",
      "Epoch 6/15\n",
      "54000/54000 [==============================] - 14s 252us/sample - loss: 0.0408 - accuracy: 0.9876 - val_loss: 0.0312 - val_accuracy: 0.9910\n",
      "Epoch 7/15\n",
      "54000/54000 [==============================] - 13s 246us/sample - loss: 0.0370 - accuracy: 0.9883 - val_loss: 0.0308 - val_accuracy: 0.9917\n",
      "Epoch 8/15\n",
      "54000/54000 [==============================] - 13s 247us/sample - loss: 0.0340 - accuracy: 0.9893 - val_loss: 0.0281 - val_accuracy: 0.9918\n",
      "Epoch 9/15\n",
      "54000/54000 [==============================] - 14s 264us/sample - loss: 0.0307 - accuracy: 0.9901 - val_loss: 0.0314 - val_accuracy: 0.9922\n",
      "Epoch 10/15\n",
      "54000/54000 [==============================] - 16s 289us/sample - loss: 0.0298 - accuracy: 0.9904 - val_loss: 0.0278 - val_accuracy: 0.9923\n",
      "Epoch 11/15\n",
      "54000/54000 [==============================] - 14s 252us/sample - loss: 0.0275 - accuracy: 0.9914 - val_loss: 0.0269 - val_accuracy: 0.9927\n",
      "Epoch 12/15\n",
      "54000/54000 [==============================] - 13s 243us/sample - loss: 0.0254 - accuracy: 0.9914 - val_loss: 0.0322 - val_accuracy: 0.9920\n",
      "Epoch 13/15\n",
      "54000/54000 [==============================] - 13s 235us/sample - loss: 0.0238 - accuracy: 0.9921 - val_loss: 0.0311 - val_accuracy: 0.9923\n",
      "Epoch 14/15\n",
      "54000/54000 [==============================] - 13s 234us/sample - loss: 0.0237 - accuracy: 0.9919 - val_loss: 0.0270 - val_accuracy: 0.9932\n",
      "Epoch 15/15\n",
      "54000/54000 [==============================] - 13s 239us/sample - loss: 0.0226 - accuracy: 0.9926 - val_loss: 0.0245 - val_accuracy: 0.9938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8d457ec390>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", \n",
    "              optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, \n",
    "          batch_size=batch_size, \n",
    "          epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.01789\n",
      "Test loss: 0.99420\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Test loss: {score[0]:.5f}')\n",
    "print(f'Test loss: {score[1]:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Superior performance versus other ANNs we've tried on the MNIST data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Data Issues\n",
    "\n",
    "- May need to augment your data with rotations, zoom in/out, crops, intensity changes.  \n",
    "- See Raschka, pages 550-564."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading and Resources \n",
    "- [TensorFlow Clothing Classification](https://www.tensorflow.org/tutorials/keras/classification)  \n",
    "- [Convolutional NN Example](https://www.tensorflow.org/tutorials/images/cnn)  \n",
    "- [Stanford CS 230 on Convolutional Neural Networks](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)\n",
    "- [DeepLearning AI Videos](https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda3e66817595a24f3b851fad3315f1c145"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
