# Week 4 - Introduction to Regression

Agenda:
- What is least squares regression?  
- Closed-form solution.  
- Evaluation metrics.  
- Reading regression output.  
- p-value hacking.  
- Checking residuals.  
- Predicting new values.  
- Robust regression methods.  
- Multiple regression. 
- Strengths and weaknesses.  
- Pipelines.  
- Ridge.
- Gradient descent.  
- Lasso.

If you are having difficulty viewing the notebook, try [Jupyter's Notebook Viewer](https://nbviewer.org).

Resources:
<br>[Ben Lambert's Derivation of OLS](https://www.youtube.com/watch?v=fb1CNQT-3Pg)
<br>[Andrew Ng Regression Lecture](https://www.youtube.com/watch?v=4b4MUYve_U8)
<br>[Cornell Regression Lecture](https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote08.html)
<br>[Deriving Ridge Regression](https://stats.stackexchange.com/questions/69205/how-to-derive-the-ridge-regression-solution)
<br>[Common Pitfalls in Interpreting Coefficients](https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html)
<br>[HBR Article - Beware Spurious Correlations](https://hbr.org/2015/06/beware-spurious-correlations)
<br>[Saving your models with pickle](https://scikit-learn.org/stable/modules/model_persistence.html)
